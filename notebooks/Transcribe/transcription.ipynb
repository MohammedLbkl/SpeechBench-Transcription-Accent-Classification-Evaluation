{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../../\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import whisper\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from jiwer import wer\n",
    "import pandas as pd\n",
    "from datasets import Dataset, Audio\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "from transformers import AutoProcessor\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset as HFDataset\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "\n",
    "def Ecoute_audio(file_path):\n",
    "  return HTML(f\"\"\"\n",
    "<audio controls>\n",
    "  <source src=\"{file_path}\" type=\"audio/mpeg\">\n",
    "  Votre navigateur ne supporte pas la balise audio.\n",
    "</audio>\n",
    "\"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vosk import Model, KaldiRecognizer\n",
    "import wave\n",
    "import json\n",
    "import whisper\n",
    "\n",
    "def transcribe(file_path, model, model_name=\"tiny\"):\n",
    "    \n",
    "    if model==\"whisper\":\n",
    "        \n",
    "        model = whisper.load_model(model_name)\n",
    "\n",
    "        result = model.transcribe(file_path)\n",
    "\n",
    "        result = result[\"text\"]\n",
    "        return result\n",
    "    \n",
    "    elif model==\"vosk\":\n",
    "        wf = wave.open(file_path, 'rb')\n",
    "        model = Model(lang=\"en-us\")\n",
    "        rec = KaldiRecognizer(model, wf.getframerate())\n",
    "        rec.SetWords(True)\n",
    "\n",
    "        full_text = \"\"\n",
    "\n",
    "        while True:\n",
    "            data = wf.readframes(4000)\n",
    "            if len(data) == 0:\n",
    "                break\n",
    "            if rec.AcceptWaveform(data):\n",
    "                result = json.loads(rec.Result())\n",
    "                \n",
    "        full_text = json.loads(rec.Result()).get(\"text\")\n",
    "        if full_text==\"\":\n",
    "            full_text = result.get(\"text\")\n",
    "\n",
    "        return full_text\n",
    "    \n",
    "    elif model==\"w2v2\":\n",
    "        df1= df.select([row])\n",
    "        dataset = Wav2Vec2Dataset(df1, processor, AUDIO_DIR)\n",
    "\n",
    "        input_values = processor(\n",
    "            dataset[0][\"input_values\"],\n",
    "            sampling_rate=16000,  \n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\"\n",
    "        ).input_values\n",
    "\n",
    "\n",
    "        logits = model_w2v2(input_values).logits\n",
    "\n",
    "\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = processor.batch_decode(predicted_ids)\n",
    "        \n",
    "        return transcription[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_transcribe(df, model, model_name = \"tiny\", iter = -1):\n",
    "\n",
    "    iter= len(df) if iter == -1 else iter\n",
    "    \n",
    "    list_score = []\n",
    "    list_duration = []        \n",
    "        \n",
    "        \n",
    "    if model==\"whisper\":\n",
    "        \n",
    "        \n",
    "        print(f\"Model: {model}/{model_name}\")  \n",
    "        \n",
    "        model = whisper.load_model(model_name)\n",
    "        \n",
    "        \n",
    "        for i in range(iter):\n",
    "            \n",
    "            if i % 3000 == 3000:\n",
    "                print(f\"Processing index: {i}\")\n",
    "                \n",
    "                \n",
    "                \n",
    "            start = time.time()\n",
    "            \n",
    "            result = model.transcribe(f'{AUDIO_DIR}/{df[\"audio\"].iloc[i]}')\n",
    "\n",
    "            end = time.time()\n",
    "\n",
    "            duration = end - start\n",
    "            \n",
    "            \n",
    "            \n",
    "            reference = df[\"transcription\"].iloc[i].lower() \n",
    "            reference = re.sub(r'[^\\w\\s]', '', reference)  \n",
    "            \n",
    "\n",
    "            hypothesis = result[\"text\"].lower() \n",
    "            hypothesis = re.sub(r'[^\\w\\s]', '', hypothesis)  \n",
    "\n",
    "            score = wer(reference, hypothesis)\n",
    "\n",
    "            list_score.append(score)\n",
    "            list_duration.append(duration)\n",
    "            \n",
    "        moyenne_duration = sum(list_duration) / len(list_duration)\n",
    "        moyenne_score = sum(list_score) / len(list_score)\n",
    "        print(f\"WER moyen: {moyenne_score:.2%}\")\n",
    "        print(f\"Durée moyenne de transcription : {moyenne_duration:.2f} secondes\")\n",
    "        \n",
    "        \n",
    "    elif model==\"vosk\":\n",
    "           \n",
    "        print(f\"Model: {model}\")\n",
    "        model = Model(lang=\"en-us\")\n",
    "\n",
    "        for i in range(iter):\n",
    "            \n",
    "            if i % 3000 == 3000:\n",
    "                print(f\"Processing index: {i}\")\n",
    "                \n",
    "                \n",
    "            wf = wave.open(f'{AUDIO_DIR}/{df[\"audio\"].iloc[i]}', 'rb')\n",
    "\n",
    "                \n",
    "            rec = KaldiRecognizer(model, wf.getframerate())\n",
    "            rec.SetWords(True)\n",
    "\n",
    "            full_text = \"\"\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            while True:\n",
    "                data = wf.readframes(4000)\n",
    "                if len(data) == 0:\n",
    "                    break\n",
    "                if rec.AcceptWaveform(data):\n",
    "                    result = json.loads(rec.Result())\n",
    "                    \n",
    "            \n",
    "            end = time.time()\n",
    "\n",
    "            duration = end - start\n",
    "            \n",
    "            full_text = json.loads(rec.Result()).get(\"text\")\n",
    "            if full_text==\"\":\n",
    "                full_text = result.get(\"text\")\n",
    "                \n",
    "            full_text = re.sub(r'[^\\w\\s]', '', full_text)  \n",
    "            hypothesis = full_text.lower()  \n",
    "\n",
    "            reference = df[\"transcription\"].iloc[i].lower()  \n",
    "            reference = re.sub(r'[^\\w\\s]', '', reference)  \n",
    "            \n",
    "            score = wer(reference, hypothesis)\n",
    "\n",
    "            list_score.append(score)\n",
    "            list_duration.append(duration)\n",
    "                \n",
    "        moyenne_duration = sum(list_duration) / len(list_duration)\n",
    "        moyenne = sum(list_score) / len(list_score)\n",
    "        \n",
    "    \n",
    "\n",
    "        print(f\"WER moyen: {moyenne:.2%}\")\n",
    "        print(f\"Durée moyenne de transcription : {moyenne_duration:.2f} secondes\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    if model==\"w2v2\":\n",
    "            \n",
    "        \n",
    "        print(f\"Model: {model}\")  \n",
    "        \n",
    "        df = Dataset.from_pandas(df)\n",
    "        df = df.cast_column(\"audio\", Audio(decode=False))\n",
    "        \n",
    "        for i in range(iter):\n",
    "            \n",
    "            if i % 3000 == 3000:\n",
    "                print(f\"Processing index: {i}\")\n",
    "                \n",
    "                \n",
    "            start = time.time()\n",
    "            df1= df.select([i])\n",
    "            dataset = Wav2Vec2Dataset(df1, processor, AUDIO_DIR)\n",
    "\n",
    "            input_values = processor(\n",
    "                dataset[0][\"input_values\"],\n",
    "                sampling_rate=16000,  \n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\"\n",
    "            ).input_values\n",
    "\n",
    "\n",
    "            logits = model_w2v2(input_values).logits\n",
    "\n",
    "\n",
    "            predicted_ids = torch.argmax(logits, dim=-1)\n",
    "            transcription = processor.batch_decode(predicted_ids)\n",
    "\n",
    "            end = time.time()\n",
    "            duration = end - start\n",
    "\n",
    "            reference = df1['transcription'][0]\n",
    "            reference = re.sub(r'[^\\w\\s]', '', reference).lower()\n",
    "            \n",
    "\n",
    "            hypothesis = transcription[0].lower()\n",
    "            \n",
    "\n",
    "            score = wer(reference, hypothesis)\n",
    "\n",
    "            list_score.append(score)\n",
    "            list_duration.append(duration) \n",
    "\n",
    "            \n",
    "        moyenne_duration = sum(list_duration) / len(list_duration)\n",
    "        moyenne_score = sum(list_score) / len(list_score)\n",
    "        print(f\"WER moyen: {moyenne_score:.2%}\")\n",
    "        print(f\"Durée moyenne de transcription : {moyenne_duration:.2f} secondes\")\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré Traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CSV = \"data/En/processed/Common Voice.csv\" \n",
    "AUDIO_DIR = \"clips_en.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>common_voice_en_40865211.wav</td>\n",
       "      <td>With this transition to the big time, the band...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>common_voice_en_40865212.wav</td>\n",
       "      <td>Local brothels recruited extra staff to cope w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>common_voice_en_40865213.wav</td>\n",
       "      <td>With Fox on lead vocals, the threesome did two...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>common_voice_en_40865214.wav</td>\n",
       "      <td>Miramax requested cuts be made and Christopher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>common_voice_en_40865215.wav</td>\n",
       "      <td>The Key allows customers to buy Plusbus for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          audio  \\\n",
       "0  common_voice_en_40865211.wav   \n",
       "1  common_voice_en_40865212.wav   \n",
       "2  common_voice_en_40865213.wav   \n",
       "3  common_voice_en_40865214.wav   \n",
       "4  common_voice_en_40865215.wav   \n",
       "\n",
       "                                       transcription  \n",
       "0  With this transition to the big time, the band...  \n",
       "1  Local brothels recruited extra staff to cope w...  \n",
       "2  With Fox on lead vocals, the threesome did two...  \n",
       "3  Miramax requested cuts be made and Christopher...  \n",
       "4  The Key allows customers to buy Plusbus for th...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = pd.read_csv(OUTPUT_CSV)\n",
    "\n",
    "metadata = metadata.dropna(subset=['wav_path', 'sentence'])\n",
    "\n",
    "metadata = metadata.rename(columns={\"wav_path\": \"audio\"})\n",
    "metadata = metadata.rename(columns={\"sentence\": \"transcription\"})\n",
    "\n",
    "metadata=metadata[[\"audio\",\"transcription\"]]\n",
    "\n",
    "metadata.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W2V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'bytes': None, 'path': 'common_voice_en_40865211.wav'},\n",
       " 'transcription': 'With this transition to the big time, the band shortened their name to Stabilo.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = Dataset.from_pandas(metadata)\n",
    "df = df.cast_column(\"audio\", Audio(decode=False))\n",
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "import os\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import librosa\n",
    "\n",
    "class Wav2Vec2Dataset(Dataset):\n",
    "    def __init__(self, hf_dataset, processor, audio_dir):\n",
    "        self.dataset = hf_dataset\n",
    "        self.processor = processor\n",
    "        self.audio_dir = audio_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.dataset[idx]\n",
    "        audio_path = os.path.join(self.audio_dir, example[\"audio\"][\"path\"])\n",
    "        speech_array, _ = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "        input_values = self.processor(speech_array, sampling_rate=16000).input_values[0]\n",
    "        labels = self.processor.tokenizer(example[\"transcription\"]).input_ids\n",
    "\n",
    "        return {\n",
    "            \"input_values\": torch.tensor(input_values),\n",
    "            \"labels\": torch.tensor(labels)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Wav2Vec2Dataset(df, processor, AUDIO_DIR)\n",
    "\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model_w2v2 = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<audio controls>\n",
       "  <source src=\"../../clips_en.wav/common_voice_en_40865211.wav\" type=\"audio/mpeg\">\n",
       "  Votre navigateur ne supporte pas la balise audio.\n",
       "</audio>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = 0\n",
    "AUDIO_PATH = f'{AUDIO_DIR}/{metadata[\"audio\"].iloc[row]}'\n",
    "file_path = f'../../{AUDIO_DIR}/{metadata[\"audio\"].iloc[row]}'\n",
    "\n",
    "    \n",
    "Ecoute_audio(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With this transition to the big time, the band shortened their name to Stabilo.\n"
     ]
    }
   ],
   "source": [
    "reference = metadata[\"transcription\"].iloc[row]\n",
    "print(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " With this transition to the big time, the band shorten their name to stay below.\n"
     ]
    }
   ],
   "source": [
    "result_whisper = transcribe(file_path=AUDIO_PATH, model=\"whisper\")\n",
    "print(result_whisper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with this transition to the big time the ban shorten their name to stay below\n"
     ]
    }
   ],
   "source": [
    "result_vosk = transcribe(file_path=AUDIO_PATH, model=\"vosk\")\n",
    "print(result_vosk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH THIS TRANSITION TO THE BIG TIME DID BAN SHORTEN THEIR NAME TO STAY BELOW\n"
     ]
    }
   ],
   "source": [
    "result_w2v2 = transcribe(file_path=AUDIO_PATH, model=\"w2v2\")\n",
    "print(result_w2v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métrique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 60.00%\n"
     ]
    }
   ],
   "source": [
    "reference_test = \"Evaluons sa capacité à transcrire.\"\n",
    "hypothesis_test = \"Evaluon sa capaciter à transcrir.\"\n",
    "\n",
    "score = wer(reference_test, hypothesis_test)\n",
    "print(f\"WER: {score:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper/ WER: 21.43%\n",
      "Vosk/ WER: 28.57%\n",
      "Wav2Vec2/ WER: 35.71%\n"
     ]
    }
   ],
   "source": [
    "reference = re.sub(r'[^\\w\\s]', '', reference).lower() # supprime tout sauf lettres/chiffres/espaces + conversion en minuscules\n",
    "\n",
    "list_result = [result_whisper, result_vosk, result_w2v2]\n",
    "list_model = [\"Whisper\", \"Vosk\", \"Wav2Vec2\"]\n",
    "\n",
    "for i in range(len(list_result)):\n",
    "    \n",
    "    hypothesis = re.sub(r'[^\\w\\s]', '', list_result[i]).lower()  \n",
    "    score = wer(reference, hypothesis)\n",
    "\n",
    "    print(f\"{list_model[i]}/ WER: {score:.2%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CommonVoice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CSV = \"data/En/processed/Common Voice.csv\" \n",
    "AUDIO_DIR = \"clips_en.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>common_voice_en_40865211.wav</td>\n",
       "      <td>With this transition to the big time, the band...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>common_voice_en_40865212.wav</td>\n",
       "      <td>Local brothels recruited extra staff to cope w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>common_voice_en_40865213.wav</td>\n",
       "      <td>With Fox on lead vocals, the threesome did two...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>common_voice_en_40865214.wav</td>\n",
       "      <td>Miramax requested cuts be made and Christopher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>common_voice_en_40865215.wav</td>\n",
       "      <td>The Key allows customers to buy Plusbus for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          audio  \\\n",
       "0  common_voice_en_40865211.wav   \n",
       "1  common_voice_en_40865212.wav   \n",
       "2  common_voice_en_40865213.wav   \n",
       "3  common_voice_en_40865214.wav   \n",
       "4  common_voice_en_40865215.wav   \n",
       "\n",
       "                                       transcription  \n",
       "0  With this transition to the big time, the band...  \n",
       "1  Local brothels recruited extra staff to cope w...  \n",
       "2  With Fox on lead vocals, the threesome did two...  \n",
       "3  Miramax requested cuts be made and Christopher...  \n",
       "4  The Key allows customers to buy Plusbus for th...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = pd.read_csv(OUTPUT_CSV)\n",
    "\n",
    "metadata = metadata.dropna(subset=['wav_path', 'sentence'])\n",
    "\n",
    "metadata = metadata.rename(columns={\"wav_path\": \"audio\"})\n",
    "metadata = metadata.rename(columns={\"sentence\": \"transcription\"})\n",
    "\n",
    "metadata=metadata[[\"audio\",\"transcription\"]]\n",
    "\n",
    "metadata.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'bytes': None, 'path': 'common_voice_en_40865211.wav'},\n",
       " 'transcription': 'With this transition to the big time, the band shortened their name to Stabilo.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = Dataset.from_pandas(metadata)\n",
    "df = df.cast_column(\"audio\", Audio(decode=False))\n",
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: whisper/tiny\n",
      "WER moyen: 10.71%\n",
      "Durée moyenne de transcription : 0.60 secondes\n"
     ]
    }
   ],
   "source": [
    "evaluate_transcribe(df=metadata ,model=\"whisper\", iter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: vosk\n",
      "WER moyen: 14.29%\n",
      "Durée moyenne de transcription : 1.17 secondes\n"
     ]
    }
   ],
   "source": [
    "evaluate_transcribe(df=metadata ,model=\"vosk\", iter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: w2v2\n",
      "WER moyen: 17.86%\n",
      "Durée moyenne de transcription : 0.35 secondes\n"
     ]
    }
   ],
   "source": [
    "evaluate_transcribe(df=metadata ,model=\"w2v2\", iter=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LibriSpeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CSV = \"data/librispeech_dataset_wav.csv\" \n",
    "AUDIO_DIR = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LibriSpeech_wav\\train-clean-360\\100\\121669\\100...</td>\n",
       "      <td>TOM THE PIPER'S SON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LibriSpeech_wav\\train-clean-360\\100\\121669\\100...</td>\n",
       "      <td>THE PIG WAS EAT AND TOM WAS BEAT AND TOM RAN C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LibriSpeech_wav\\train-clean-360\\100\\121669\\100...</td>\n",
       "      <td>HE NEVER DID ANY WORK EXCEPT TO PLAY THE PIPES...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LibriSpeech_wav\\train-clean-360\\100\\121669\\100...</td>\n",
       "      <td>BUT HE WAS SO SLY AND CAUTIOUS THAT NO ONE HAD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LibriSpeech_wav\\train-clean-360\\100\\121669\\100...</td>\n",
       "      <td>AND THEY LIVED ALL ALONE IN A LITTLE HUT AWAY ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               audio  \\\n",
       "0  LibriSpeech_wav\\train-clean-360\\100\\121669\\100...   \n",
       "1  LibriSpeech_wav\\train-clean-360\\100\\121669\\100...   \n",
       "2  LibriSpeech_wav\\train-clean-360\\100\\121669\\100...   \n",
       "3  LibriSpeech_wav\\train-clean-360\\100\\121669\\100...   \n",
       "4  LibriSpeech_wav\\train-clean-360\\100\\121669\\100...   \n",
       "\n",
       "                                       transcription  \n",
       "0                                TOM THE PIPER'S SON  \n",
       "1  THE PIG WAS EAT AND TOM WAS BEAT AND TOM RAN C...  \n",
       "2  HE NEVER DID ANY WORK EXCEPT TO PLAY THE PIPES...  \n",
       "3  BUT HE WAS SO SLY AND CAUTIOUS THAT NO ONE HAD...  \n",
       "4  AND THEY LIVED ALL ALONE IN A LITTLE HUT AWAY ...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = pd.read_csv(OUTPUT_CSV)\n",
    "\n",
    "metadata = metadata.rename(columns={\"wav_path\": \"audio\"})\n",
    "\n",
    "metadata = metadata.dropna(subset=['audio', 'transcription'])\n",
    "\n",
    "metadata=metadata[[\"audio\",\"transcription\"]]\n",
    "\n",
    "metadata.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'bytes': None,\n",
       "  'path': 'LibriSpeech_wav\\\\train-clean-360\\\\100\\\\121669\\\\100-121669-0000.wav'},\n",
       " 'transcription': \"TOM THE PIPER'S SON\"}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = Dataset.from_pandas(metadata)\n",
    "df = df.cast_column(\"audio\", Audio(decode=False))\n",
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: whisper/tiny\n",
      "WER moyen: 31.67%\n",
      "Durée moyenne de transcription : 0.53 secondes\n"
     ]
    }
   ],
   "source": [
    "evaluate_transcribe(df=metadata ,model=\"whisper\", iter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: vosk\n",
      "WER moyen: 19.17%\n",
      "Durée moyenne de transcription : 1.03 secondes\n"
     ]
    }
   ],
   "source": [
    "evaluate_transcribe(df=metadata ,model=\"vosk\", iter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: w2v2\n",
      "WER moyen: 12.50%\n",
      "Durée moyenne de transcription : 0.27 secondes\n"
     ]
    }
   ],
   "source": [
    "evaluate_transcribe(df=metadata ,model=\"w2v2\", iter=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2-ARCTIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CSV = \"data/L2-ARTICLE.csv\" \n",
    "AUDIO_DIR = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data4\\ABA\\wav\\arctic_a0001.wav</td>\n",
       "      <td>Author of the danger trail Philip Steels etc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data4\\ABA\\wav\\arctic_a0002.wav</td>\n",
       "      <td>Not at this particular case Tom apologized Whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data4\\ABA\\wav\\arctic_a0003.wav</td>\n",
       "      <td>For the twentieth time that evening the two me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data4\\ABA\\wav\\arctic_a0004.wav</td>\n",
       "      <td>Lord but I'm glad to see you again Phil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data4\\ABA\\wav\\arctic_a0005.wav</td>\n",
       "      <td>Will we ever forget it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            audio  \\\n",
       "0  data4\\ABA\\wav\\arctic_a0001.wav   \n",
       "1  data4\\ABA\\wav\\arctic_a0002.wav   \n",
       "2  data4\\ABA\\wav\\arctic_a0003.wav   \n",
       "3  data4\\ABA\\wav\\arctic_a0004.wav   \n",
       "4  data4\\ABA\\wav\\arctic_a0005.wav   \n",
       "\n",
       "                                       transcription  \n",
       "0       Author of the danger trail Philip Steels etc  \n",
       "1  Not at this particular case Tom apologized Whi...  \n",
       "2  For the twentieth time that evening the two me...  \n",
       "3            Lord but I'm glad to see you again Phil  \n",
       "4                             Will we ever forget it  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = pd.read_csv(OUTPUT_CSV)\n",
    "\n",
    "metadata = metadata.dropna(subset=['audio', 'transcription'])\n",
    "metadata[\"audio\"] = metadata[\"audio\"].str.replace(r\"^\\.\\.\\\\\", \"\", regex=True)\n",
    "\n",
    "metadata=metadata[[\"audio\",\"transcription\"]]\n",
    "\n",
    "metadata.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'bytes': None, 'path': 'data4\\\\ABA\\\\wav\\\\arctic_a0001.wav'},\n",
       " 'transcription': 'Author of the danger trail Philip Steels etc'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = Dataset.from_pandas(metadata)\n",
    "df = df.cast_column(\"audio\", Audio(decode=False))\n",
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: whisper/tiny\n",
      "WER moyen: 50.00%\n",
      "Durée moyenne de transcription : 0.53 secondes\n"
     ]
    }
   ],
   "source": [
    "evaluate_transcribe(df=metadata ,model=\"whisper\", iter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: vosk\n",
      "WER moyen: 43.75%\n",
      "Durée moyenne de transcription : 1.08 secondes\n"
     ]
    }
   ],
   "source": [
    "evaluate_transcribe(df=metadata ,model=\"vosk\", iter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: w2v2\n",
      "WER moyen: 50.00%\n",
      "Durée moyenne de transcription : 0.32 secondes\n"
     ]
    }
   ],
   "source": [
    "evaluate_transcribe(df=metadata ,model=\"w2v2\", iter=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model: W2V2 \n",
    "\n",
    "WER moyen: 49.61%\n",
    "\n",
    "Durée moyenne de transcription : 0.54 secondes\n",
    "\n",
    "- Model: whisper/tiny\n",
    "\n",
    "WER moyen: 49.04%\n",
    "\n",
    "Durée moyenne de transcription : 1.32 secondes\n",
    "\n",
    "- Model: vosk\n",
    "\n",
    "WER moyen: 50.66%\n",
    "\n",
    "Durée moyenne de transcription : 1.40 secondes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
